{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/evillag/TEC-ML-2022-1/blob/main/TP2/src_base/TP2_BayesianModel_MNIST_EVG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Curso de Aprendizaje Automático\n",
        "# Trabajo Practico 2: Clasificador Bayes Ingenuo\n",
        "**Escuela de Ingeniería en Computación | Instituto Tecnológico de Costa Rica**\n",
        "\n",
        "Realizado por\n",
        "*   Luis Badilla Ortiz\n",
        "*   William Jiménez García\n",
        "*   Esteban Villalobos Gómez\n",
        "\n",
        "Fecha de entrega\n",
        "*  15 de Mayo de 2022\n",
        "\n",
        "Entrega\n",
        "* Un archivo .zip con el código fuente LaTeX o Lyx, el pdf, y un notebook en jupyter, debidamente documentado, con una función definida por ejercicio. A través del TEC-digital.\n",
        "\n",
        "Modo de trabajo\n",
        "* Grupos de 3 personas.\n",
        "\n",
        "En el presente trabajo práctico se introduce la implementación de redes bayesianas.\n",
        "________________________\n"
      ],
      "metadata": {
        "id": "80WG6jNSABbP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A. Respuestas y análisis de resultados\n",
        "\n",
        "3) **Entrene el modelo con el conjunto de observaciones contenido en la carpeta train, y reporte la tasa de aciertos al utilizar la función anteriormente implementada test_model_batch (se espera que la tasa de aciertos sea mayor a 80 %). Verifique y comente los resultados.**\n",
        "<br><br>\n",
        "R/ Model accuracy: $91.6667$%\n",
        "<br><br>\n",
        "Al realizar el entrenamiento y evaluación del modelo se obtuvo una tasa de aciertos del 91.6%, lo cual indica un alto grado de precisión. Luego de una revisión detallada de los datos y el score calculado en cada caso se pudo observar que las imagenes que fueron etiquetadas incorrectamente son dificiles de identificar incluso para el ojo humano, como la imagen de abajo que esta etiquetada como 1 y Bayes estimó que es un 8.\n",
        "\n",
        "![8.png](data:image/png;base64,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)\n",
        "\n",
        "<br>\n",
        "\n",
        "3) **(cont.) ... Si observa valores de 0 o nulos en la evaluación de la función, argumente el porqué puede deberse este comportamiento. ¿Cómo se puede corregir el problema detectado, según las herramientas matemáticas estudiadas en clase? Implemente tal enfoque y compruebe los resultados.**\n",
        "<br><br>\n",
        "R/ Al realizar la multiplicatoria de ∏ p(m|t) * p(t) se pudo observar que los valores eran muy bajos y llegaban a dar un resultado de cero, lo cual anulaba completamente el algoritmo. A esto se le conoce como un problema de underflow que resulta de calcular el producto de muchos números de magnitud menor que uno. \n",
        "<br><br>\n",
        "Para evitar este problema se optó por calcular el logaritmo natural de la función de verosimilitud, lo cual convierte la multiplicatoria en una sumatoria de logaritmos naturales y dado que la sumatoria no decrese la magnitud del resultado, el problema de underflow se puede evitar. Lo que se busca es obtener el valor máximo por lo que la transformación logaritmica no afecta el resultado.\n",
        "<br><br>\n",
        "\n",
        "\n",
        "4) **Particione los datos de forma aleatoria con 70 % de las observaciones para entrenamiento y 30 % para prueba (a partir de la carpeta train). Calcule la tasa de aciertos para 10 corridas, cada una con una partición de entrenamiento y otra de prueba distintas. Reporte los resultados de las corridas en una tabla, además de la media y desviación estándar de la tasa de aciertos para las 10 corridas. Para realizar las particiones puede usar la libreria sklearn.**\n",
        "\n",
        "R/ \n",
        "\n",
        "| Corrida | Accuracy |\n",
        "|---------|---------|\n",
        "|    1    | 0.4222 |\n",
        "|    2    | 0.3944 |\n",
        "|    3    | 0.4333 |\n",
        "|    4    | 0.3888 |\n",
        "|    5    | 0.4888 |\n",
        "|    6    | 0.5111 |\n",
        "|    7    | 0.3555 |\n",
        "|    8    | 0.5111 |\n",
        "|    9    | 0.4000 |\n",
        "|   10    | 0.4833 |\n",
        "| PROMEDIO| 0.4389 |\n",
        "| S.D.    | 0.0559 |\n",
        "\n",
        "<br>\n",
        "\n",
        "Realizando estas pruebas se puede notar una gran disminución en el accuracy del modelo, esto puede deberse al desbalanceo de las muestras al momento de realizar el entrenamiento por la selección aleatoria de muestras en cada partición lo que aumenta las probabilidades para aquellos números que posean más muestras ocasionando que tengan más peso en las probabilidades al realizar una estimación.\n",
        "\n",
        "<br>\n",
        "\n",
        "5) **Estudie el efecto del desbalanceo de los datos descrito en la Tabla 1, como escenario. Realice 10 particiones de forma aleatoria, y calcula la media y desviacion estandar de los resultados. Compare y comente los resultados respecto a los resultados en el punto anterior con el dataset balanceado.**\n",
        "\n",
        "R/\n",
        "\n",
        "| Corrida | Accuracy |\n",
        "|---------|--------|\n",
        "|    1    | 0.3944 |\n",
        "|    2    | 0.3777 |\n",
        "|    3    | 0.3777 |\n",
        "|    4    | 0.4000 |\n",
        "|    5    | 0.4500 |\n",
        "|    6    | 0.4111 |\n",
        "|    7    | 0.4111 |\n",
        "|    8    | 0.4222 |\n",
        "|    9    | 0.3722 |\n",
        "|   10    | 0.4500 |\n",
        "| PROMEDIO| 0.4066 |\n",
        "| S.D.    | 0.0266 |\n",
        "\n",
        "<br>\n",
        "\n",
        "Realizando las pruebas con el desbalanceo de las muestras para entrenamiento se observa un comportamiento similar al que ocurre con la generación de particiones, es decir el accuracy del modelo disminuye considerablemente en comparación al modelo entrenado con un dataset balanceado, esto puede deberse a las desigualdades en las probabilidades que pueden ocasionar que algunas clases tengan más peso al realizar las estimaciones del modelo.\n",
        "\n",
        "<br>\n",
        "\n",
        "Un punto a resaltar es que la desviación estándar del modelo con datos desbalanceados es menor a la obtenida al particionar los datos, esto puede deberse a que en el desbalanceo aún mantenemos una proporción equivalente entre las muestras de las clases en cada iteración sin embargo en el particionamiento esta proporción puede variar debido a que la selección es aleatoria por cada split.\n"
      ],
      "metadata": {
        "id": "z1gxVIIbAwS3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "________________________\n",
        "\n",
        "## B. Código fuente"
      ],
      "metadata": {
        "id": "FBLarF9oA-9h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "xABNNGBFGail"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "fV91T7yxTRaL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "import torchvision.transforms.functional as TF\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from sklearn.model_selection import ShuffleSplit"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get Data"
      ],
      "metadata": {
        "id": "PchQ0WrwGcsN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!wget https://github.com/evillag/TEC-ML-2022-1/raw/main/TP2/src_base/mnist_dataset.zip\n",
        "!unzip /content/mnist_dataset.zip"
      ],
      "metadata": {
        "id": "CD_niHaQTuJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Data\n",
        "####Punto: 1)\n"
      ],
      "metadata": {
        "id": "m5vsDDsQGe5y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "vXz83R23TRaM"
      },
      "outputs": [],
      "source": [
        "def binarize_image(image_tensor):\n",
        "    image_tensor[image_tensor > 0.5] = 1\n",
        "    image_tensor[image_tensor <= 0.5] = 0\n",
        "    return image_tensor\n",
        "\n",
        "def load_dataset(path = \"mnist_dataset/train\"):\n",
        "    #Open up the dataset\n",
        "    dataset =  torchvision.datasets.ImageFolder(path)   \n",
        "    list_images = dataset.imgs\n",
        "    train_data_tensor = None    \n",
        "    labels_training = []\n",
        "    first_tensor = True\n",
        "    for i in range(len(list_images)):\n",
        "        pair_path_label = list_images[i]        \n",
        "        image = Image.open(pair_path_label[0]) \n",
        "        x_tensor = TF.to_tensor(image).squeeze()\n",
        "        x_tensor_bin = binarize_image(x_tensor)\n",
        "        x_tensor_bin_plain = x_tensor_bin.view(x_tensor_bin.shape[0] * x_tensor_bin.shape[1], -1)           \n",
        "        labels_training += [pair_path_label[1]]\n",
        "        if(first_tensor):\n",
        "            #plt.figure()\n",
        "            #plt.imshow(x_tensor_bin)\n",
        "            first_tensor = False\n",
        "            train_data_tensor = x_tensor_bin_plain\n",
        "        else:\n",
        "            train_data_tensor = torch.cat((train_data_tensor, x_tensor_bin_plain), 1)    \n",
        "    return (train_data_tensor, torch.tensor(labels_training))            "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Data\n",
        "print(\"LOADING DATA\")\n",
        "(train_data_tensor, labels_training) = load_dataset(path = \"/content/mnist_dataset/train\")\n",
        "print(\"Train data dimensions: \", train_data_tensor.shape)\n",
        "print(\"Train labels dimensions: \", len(labels_training))  \n",
        "print(\"LOADING DATA: COMPLETE\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUyV5huBLQAi",
        "outputId": "85403e39-054b-4cf1-af46-f63ed98a0a81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOADING DATA\n",
            "Train data dimensions:  torch.Size([784, 600])\n",
            "Train labels dimensions:  600\n",
            "LOADING DATA: COMPLETE\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Model\n",
        "#### Puntos: 2.a) 2.b) 2.c)"
      ],
      "metadata": {
        "id": "jrNaBhPfHIBc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def train_model(train_data_tensor, labels_training, num_classes = 10):\n",
        "  p_m_0_given_k = torch.zeros(num_classes, train_data_tensor.shape[0])\n",
        "  p_m_1_given_k = torch.ones(num_classes, train_data_tensor.shape[0])\n",
        "  \n",
        "  _, counts = torch.unique(labels_training, return_counts=True)\n",
        "  p_t_tensor = counts / len(labels_training)\n",
        "\n",
        "  for k in range(num_classes):\n",
        "    training_indices_of_class_k = (labels_training == k).nonzero().squeeze(1)\n",
        "    M_d = train_data_tensor[:, training_indices_of_class_k]\n",
        "\n",
        "    # p(m = 0 | k) =>  (observations == 0) / (total observations of class k)\n",
        "    p_m_0_given_k[k, :] = (M_d == 0.).sum(dim=1).div(M_d.shape[1])\n",
        "\n",
        "  # calculate complement => p(m = 1 | k)\n",
        "  p_m_1_given_k = p_m_1_given_k - p_m_0_given_k\n",
        "\n",
        "  return [p_m_0_given_k, p_m_1_given_k], p_t_tensor"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "FRo23D7FTRaO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Model\n",
        "#### Punto: 2.d)"
      ],
      "metadata": {
        "id": "xMmj95sJGsM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(input_torch, p_m_pix_val_given_k, p_t_tensor, \n",
        "                               num_classes = 10):\n",
        "    # assumes that the input comes in a row\n",
        "    # Since the multiplicatory tends to give very small values, close to 0\n",
        "    # We are transforming the Naive Bayes by applying a log to it, so that:\n",
        "    # log(Mul(p(m|t)p(t))) = Sum(log(p(m|t)) + log(p(t)))\n",
        "\n",
        "    prob_naive_bayes = torch.zeros(num_classes, train_data_tensor.shape[0])\n",
        "    scores_classes = torch.zeros(10)\n",
        "\n",
        "    idx_0 = (input_torch == 0.) # indices of all pixels in 0\n",
        "    idx_1 = (input_torch == 1.) # indices of all pixels in 1\n",
        "\n",
        "    # probability of pixels = 0 given k:\n",
        "    prob_naive_bayes[:, idx_0] = torch.log(p_m_pix_val_given_k[0][:, idx_0]) + torch.log(p_t_tensor.view(num_classes, 1))\n",
        "\n",
        "    # probability of pixels = 1 given k:\n",
        "    prob_naive_bayes[:, idx_1] = torch.log(p_m_pix_val_given_k[1][:, idx_1]) + torch.log(p_t_tensor.view(num_classes, 1))\n",
        "\n",
        "    # calculate the class probability\n",
        "    scores_classes = torch.sum(prob_naive_bayes, 1)\n",
        "\n",
        "    values, indices = scores_classes.topk(1)\n",
        "\n",
        "    predicted_label = indices.item()\n",
        "\n",
        "    #print(f'predicted_label: {indices[0]}. Score = {values[0]}')\n",
        "    \n",
        "    return predicted_label, scores_classes"
      ],
      "metadata": {
        "id": "bky9xifJG1Kb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Model Batch\n",
        "#### Punto: 2.e)"
      ],
      "metadata": {
        "id": "0aaJWK9dGwDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model_batch(test_set, labels, p_m_pix_val_given_k, p_t_tensor):\n",
        "    correct = 0\n",
        "    \n",
        "    for i in range(labels.shape[0]):\n",
        "      predicted_label, _ = test_model(test_set[:,i], p_m_pix_val_given_k, \n",
        "                                      p_t_tensor)\n",
        "      correct += 1 if predicted_label == labels[i] else 0\n",
        "      # print(f'predicted = {predicted_label}. real = {labels[i]}')\n",
        "\n",
        "    accuracy = correct / labels.shape[0]\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "vtEJxedOGvod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training model and testing using batches\n",
        "#### Punto: 3)"
      ],
      "metadata": {
        "id": "L3VnY-5JLWIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Training model by calculating the prior probabilities\n",
        "print(\"TRAINING MODEL\")\n",
        "(p_m_pix_val_given_k, p_t_tensor) = train_model(train_data_tensor, labels_training)\n",
        "print(f\"p_m_pix_val_given_k[0] size {p_m_pix_val_given_k[0].shape}\")\n",
        "print(f\"p_m_pix_val_given_k[1] size {p_m_pix_val_given_k[1].shape}\")\n",
        "print(f\"p_t_tensor: {p_t_tensor}\")\n",
        "print(\"TRAINING MODEL: COMPLETE\\n\")\n",
        "\n",
        "# Testing model by batchs\n",
        "print(\"TESTING MODEL, Batchs\")\n",
        "acc = test_model_batch(train_data_tensor, labels_training, p_m_pix_val_given_k, p_t_tensor)\n",
        "print(f\"Model accuracy: {round(acc * 100, 4)}%\")\n",
        "print(\"TESTING MODEL, Batchs: COMPLETE\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YH53ZlsBHSBL",
        "outputId": "e7fcbe4f-2013-4864-f26d-0a8cbf9147b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAINING MODEL\n",
            "p_m_pix_val_given_k[0] size torch.Size([10, 784])\n",
            "p_m_pix_val_given_k[1] size torch.Size([10, 784])\n",
            "p_t_tensor: tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
            "        0.1000])\n",
            "TRAINING MODEL: COMPLETE\n",
            "\n",
            "TESTING MODEL, Batchs\n",
            "Model accuracy: 91.6667%\n",
            "TESTING MODEL, Batchs: COMPLETE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Model Batch: Partitions\n",
        "#### Punto: 4)"
      ],
      "metadata": {
        "id": "ndE7RTojM8Os"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def partition_validation(train_data_tensor, labels_data_tensor, num_splits):\n",
        "  \"\"\"\n",
        "  Create and test dataset partitions\n",
        "  \"\"\"\n",
        "\n",
        "  # Shuffle Split\n",
        "  shuffle_split = ShuffleSplit(n_splits=num_splits, test_size=.30) \n",
        "  \n",
        "  # Iteration counter\n",
        "  iteration = 1\n",
        "\n",
        "  # Results\n",
        "  results = torch.zeros(num_splits)\n",
        "\n",
        "  train_data_tensor = train_data_tensor.T\n",
        "\n",
        "  for train_index, test_index in shuffle_split.split(train_data_tensor):\n",
        "\n",
        "    print(f\"Iteration: {iteration}\")\n",
        "    \n",
        "    # Get Train Data\n",
        "    train_torch = train_data_tensor[train_index, :]\n",
        "    labels_training = labels_data_tensor[train_index]\n",
        "\n",
        "    # Get Test Data\n",
        "    test_torch = train_data_tensor[test_index]\n",
        "    labels_testing = labels_data_tensor[test_index]\n",
        "\n",
        "    # TRAIN\n",
        "    print(\"===== TRAINING =====\")\n",
        "    (p_m_pix_val_given_k, p_t_tensor) = train_model(train_torch.T, labels_training)\n",
        "    print(p_t_tensor)\n",
        "    \n",
        "    # TEST\n",
        "    print(\"===== TESTING =====\")\n",
        "    acc = test_model_batch(test_torch.T, labels_testing, p_m_pix_val_given_k, p_t_tensor) # Test with test data\n",
        "    #acc = test_model_batch(train_torch.T, labels_training, p_m_pix_val_given_k, p_t_tensor) # Test with train data\n",
        "    print(f\"Model accuracy: {round(acc * 100, 4)}%\\n\")\n",
        "\n",
        "    # Append Accuracy Result\n",
        "    results[iteration - 1] = (round(acc, 4))\n",
        "\n",
        "    # Add +1 to current iteration\n",
        "    iteration += 1\n",
        "\n",
        "  return results\n",
        "\n",
        "accuracies = partition_validation(train_data_tensor, labels_training, 10)\n",
        "print(\"===== RESULT =====\")\n",
        "print(\"Accuracies: \", accuracies)\n",
        "print(\"Mean: \", accuracies.mean())\n",
        "print(\"Std: \", accuracies.std())"
      ],
      "metadata": {
        "id": "1xbNLYOXOvp7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c08a339f-008a-4706-988b-6e8a0138d2ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 1\n",
            "===== TRAINING =====\n",
            "tensor([0.0929, 0.0833, 0.1071, 0.1071, 0.1000, 0.1000, 0.1143, 0.1000, 0.0905,\n",
            "        0.1048])\n",
            "===== TESTING =====\n",
            "Model accuracy: 43.3333%\n",
            "\n",
            "Iteration: 2\n",
            "===== TRAINING =====\n",
            "tensor([0.1024, 0.1048, 0.1119, 0.1024, 0.1024, 0.0952, 0.1000, 0.0976, 0.0929,\n",
            "        0.0905])\n",
            "===== TESTING =====\n",
            "Model accuracy: 47.7778%\n",
            "\n",
            "Iteration: 3\n",
            "===== TRAINING =====\n",
            "tensor([0.1095, 0.0881, 0.1071, 0.1071, 0.0952, 0.0976, 0.0905, 0.1048, 0.1000,\n",
            "        0.1000])\n",
            "===== TESTING =====\n",
            "Model accuracy: 44.4444%\n",
            "\n",
            "Iteration: 4\n",
            "===== TRAINING =====\n",
            "tensor([0.0905, 0.1048, 0.0952, 0.1024, 0.0976, 0.1048, 0.0929, 0.1119, 0.1024,\n",
            "        0.0976])\n",
            "===== TESTING =====\n",
            "Model accuracy: 51.1111%\n",
            "\n",
            "Iteration: 5\n",
            "===== TRAINING =====\n",
            "tensor([0.1048, 0.1024, 0.1048, 0.1119, 0.0952, 0.1048, 0.0905, 0.0952, 0.0976,\n",
            "        0.0929])\n",
            "===== TESTING =====\n",
            "Model accuracy: 48.3333%\n",
            "\n",
            "Iteration: 6\n",
            "===== TRAINING =====\n",
            "tensor([0.1167, 0.0857, 0.0976, 0.1000, 0.1024, 0.1071, 0.1024, 0.0976, 0.0881,\n",
            "        0.1024])\n",
            "===== TESTING =====\n",
            "Model accuracy: 37.7778%\n",
            "\n",
            "Iteration: 7\n",
            "===== TRAINING =====\n",
            "tensor([0.0952, 0.1071, 0.0952, 0.1095, 0.1000, 0.1048, 0.1024, 0.1024, 0.0857,\n",
            "        0.0976])\n",
            "===== TESTING =====\n",
            "Model accuracy: 50.5556%\n",
            "\n",
            "Iteration: 8\n",
            "===== TRAINING =====\n",
            "tensor([0.0976, 0.1000, 0.0929, 0.0929, 0.1071, 0.0976, 0.0976, 0.1048, 0.1024,\n",
            "        0.1071])\n",
            "===== TESTING =====\n",
            "Model accuracy: 53.8889%\n",
            "\n",
            "Iteration: 9\n",
            "===== TRAINING =====\n",
            "tensor([0.0905, 0.1095, 0.1119, 0.1024, 0.1071, 0.0929, 0.1119, 0.0881, 0.0976,\n",
            "        0.0881])\n",
            "===== TESTING =====\n",
            "Model accuracy: 48.3333%\n",
            "\n",
            "Iteration: 10\n",
            "===== TRAINING =====\n",
            "tensor([0.0976, 0.0929, 0.1071, 0.1071, 0.1024, 0.1095, 0.0929, 0.1024, 0.0881,\n",
            "        0.1000])\n",
            "===== TESTING =====\n",
            "Model accuracy: 38.3333%\n",
            "\n",
            "===== RESULT =====\n",
            "Accuracies:  tensor([0.4333, 0.4778, 0.4444, 0.5111, 0.4833, 0.3778, 0.5056, 0.5389, 0.4833,\n",
            "        0.3833])\n",
            "Mean:  tensor(0.4639)\n",
            "Std:  tensor(0.0535)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Model: Unbalanced Data\n",
        "#### Punto: 5)"
      ],
      "metadata": {
        "id": "9MLQzgDYOcr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_unbalanced_data_set(train_data_tensor, labels_training, num_classes):\n",
        "  no_test_samples = 18\n",
        "  no_train_samples = [22, 22, 22, 22, 22, 42, 42, 42, 42, 42]\n",
        "\n",
        "  train_data = torch.tensor([])\n",
        "  train_labels = torch.tensor([])\n",
        "\n",
        "  test_data = torch.tensor([])\n",
        "  test_labels = torch.tensor([])\n",
        "\n",
        "  for current_class in range(num_classes):\n",
        "    train_data_current_class = train_data_tensor[:, labels_training == current_class]\n",
        "    labels_current_class = labels_training[labels_training == current_class]\n",
        "\n",
        "    # Indexes to create a random dataset\n",
        "    indexes = [i for i in range(train_data_current_class.shape[1])]\n",
        "    random.shuffle(indexes)\n",
        "\n",
        "    # Test Data\n",
        "    indexes_test_data = indexes[:no_test_samples] # Get data from [0, 18[\n",
        "    test_data_temp = train_data_current_class[:, indexes_test_data]\n",
        "    test_data = torch.cat((test_data, test_data_temp), 1)\n",
        "    \n",
        "    # Test Labels\n",
        "    test_labels = torch.cat((test_labels, labels_current_class[indexes_test_data])) \n",
        "\n",
        "    # Train Data\n",
        "    indexes_train_data = indexes[no_test_samples: no_test_samples + no_train_samples[current_class]] # Get data from [18, (40 || 60)[\n",
        "    train_data_temp = train_data_current_class[:, indexes_train_data]\n",
        "    train_data = torch.cat((train_data, train_data_temp), 1)\n",
        "\n",
        "    # Train Labels\n",
        "    train_labels = torch.cat((train_labels, labels_current_class[indexes_train_data]))\n",
        "\n",
        "  #print((train_data, train_labels, test_data, test_labels))\n",
        "  return (train_data, train_labels, test_data, test_labels)\n",
        "\n",
        "# Variables\n",
        "num_classes = 10\n",
        "accuracies = torch.zeros(num_classes)\n",
        "\n",
        "# Create 10 partitions\n",
        "for current_class in range(num_classes):\n",
        "\n",
        "  print(f\"=== Iteration: {current_class + 1} ===\")\n",
        "\n",
        "  # Create Data\n",
        "  (train_data, train_labels, test_data, test_labels) = create_unbalanced_data_set(train_data_tensor, labels_training, num_classes)\n",
        "\n",
        "  # TRAIN\n",
        "  print(\"===== TRAINING =====\")\n",
        "  (p_m_pix_val_given_k, p_t_tensor) = train_model(train_data, train_labels)\n",
        "  print(\"p_t tensor:\", p_t_tensor)\n",
        "\n",
        "  # TEST\n",
        "  print(\"===== TESTING =====\")\n",
        "  acc = test_model_batch(test_data, test_labels, p_m_pix_val_given_k, p_t_tensor) # Test with test data\n",
        "  #acc = test_model_batch(train_torch.T, labels_training, p_m_pix_val_given_k, p_t_tensor) # Test with train data\n",
        "  print(f\"Model accuracy: {round(acc * 100, 4)}%\\n\")\n",
        "\n",
        "  accuracies[current_class] = acc\n",
        "\n",
        "print(\"===== RESULT =====\")\n",
        "print(\"Accuracies: \", accuracies)\n",
        "print(\"Mean: \", accuracies.mean())\n",
        "print(\"Std: \", accuracies.std())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UY0mVF1oO86n",
        "outputId": "42f1e5a4-97c2-445c-978e-4e9b14c8c3d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Iteration: 1 ===\n",
            "===== TRAINING =====\n",
            "p_t tensor: tensor([0.0688, 0.0688, 0.0688, 0.0688, 0.0688, 0.1312, 0.1312, 0.1312, 0.1312,\n",
            "        0.1312])\n",
            "===== TESTING =====\n",
            "Model accuracy: 35.5556%\n",
            "\n",
            "=== Iteration: 2 ===\n",
            "===== TRAINING =====\n",
            "p_t tensor: tensor([0.0688, 0.0688, 0.0688, 0.0688, 0.0688, 0.1312, 0.1312, 0.1312, 0.1312,\n",
            "        0.1312])\n",
            "===== TESTING =====\n",
            "Model accuracy: 42.7778%\n",
            "\n",
            "=== Iteration: 3 ===\n",
            "===== TRAINING =====\n",
            "p_t tensor: tensor([0.0688, 0.0688, 0.0688, 0.0688, 0.0688, 0.1312, 0.1312, 0.1312, 0.1312,\n",
            "        0.1312])\n",
            "===== TESTING =====\n",
            "Model accuracy: 47.2222%\n",
            "\n",
            "=== Iteration: 4 ===\n",
            "===== TRAINING =====\n",
            "p_t tensor: tensor([0.0688, 0.0688, 0.0688, 0.0688, 0.0688, 0.1312, 0.1312, 0.1312, 0.1312,\n",
            "        0.1312])\n",
            "===== TESTING =====\n",
            "Model accuracy: 42.2222%\n",
            "\n",
            "=== Iteration: 5 ===\n",
            "===== TRAINING =====\n",
            "p_t tensor: tensor([0.0688, 0.0688, 0.0688, 0.0688, 0.0688, 0.1312, 0.1312, 0.1312, 0.1312,\n",
            "        0.1312])\n",
            "===== TESTING =====\n",
            "Model accuracy: 43.8889%\n",
            "\n",
            "=== Iteration: 6 ===\n",
            "===== TRAINING =====\n",
            "p_t tensor: tensor([0.0688, 0.0688, 0.0688, 0.0688, 0.0688, 0.1312, 0.1312, 0.1312, 0.1312,\n",
            "        0.1312])\n",
            "===== TESTING =====\n",
            "Model accuracy: 43.8889%\n",
            "\n",
            "=== Iteration: 7 ===\n",
            "===== TRAINING =====\n",
            "p_t tensor: tensor([0.0688, 0.0688, 0.0688, 0.0688, 0.0688, 0.1312, 0.1312, 0.1312, 0.1312,\n",
            "        0.1312])\n",
            "===== TESTING =====\n",
            "Model accuracy: 46.6667%\n",
            "\n",
            "=== Iteration: 8 ===\n",
            "===== TRAINING =====\n",
            "p_t tensor: tensor([0.0688, 0.0688, 0.0688, 0.0688, 0.0688, 0.1312, 0.1312, 0.1312, 0.1312,\n",
            "        0.1312])\n",
            "===== TESTING =====\n",
            "Model accuracy: 38.3333%\n",
            "\n",
            "=== Iteration: 9 ===\n",
            "===== TRAINING =====\n",
            "p_t tensor: tensor([0.0688, 0.0688, 0.0688, 0.0688, 0.0688, 0.1312, 0.1312, 0.1312, 0.1312,\n",
            "        0.1312])\n",
            "===== TESTING =====\n",
            "Model accuracy: 47.2222%\n",
            "\n",
            "=== Iteration: 10 ===\n",
            "===== TRAINING =====\n",
            "p_t tensor: tensor([0.0688, 0.0688, 0.0688, 0.0688, 0.0688, 0.1312, 0.1312, 0.1312, 0.1312,\n",
            "        0.1312])\n",
            "===== TESTING =====\n",
            "Model accuracy: 40.0%\n",
            "\n",
            "===== RESULT =====\n",
            "Accuracies:  tensor([0.3556, 0.4278, 0.4722, 0.4222, 0.4389, 0.4389, 0.4667, 0.3833, 0.4722,\n",
            "        0.4000])\n",
            "Mean:  tensor(0.4278)\n",
            "Std:  tensor(0.0390)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "TP2_BayesianModel_MNIST_LB_WJ_EV.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}